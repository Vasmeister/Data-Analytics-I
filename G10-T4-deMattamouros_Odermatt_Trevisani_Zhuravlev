# ----------------------------------------------------------------------------
#                                                                                                                      
#
#                  88
#                  88
#                  88             
#                  88                                   
#                  88                                    
#                  8b,dPPPPYba, ,dPPPYba,   ,adYPPPYba88
#                  88P'    `"8a a8P   `"8a  a8P       88
#                  88        88 8PP         8PP       88
#                  88        88 `"PPYYba"   8O        88
#                  88        88         8P   8b,     ,88
#                  88        88 "8a,   "a8   `"Ybbdd888"
#                  88        88 `"YbbdPP"             88
#                                                     88
#                                            "8b      88
#                                            `"Ybbdd888"  
#
#     Jose de Mattamouros, Linda Odermatt, Elena Trevisani, Vasily Zhuravlev
#     Master in Economics
#     University of St. Gallen
#     9000 St. Gallen
#
# ------------------------------------------------------------------------------
# 
# Filename:  Data Analytics 1: Predictive Econometrics - PC 4
# Topic:     Forests
# Date of submission: 11.12.2022
#
# ------------------------------------------------------------------------------
#Step 1: Set the working directory:
#Find which directory you are currently working in
ddpath 
getwd()
# "~/Desktop/HSG/MEcon Herbst Semester/2nd half of the semester/Data Analytics I - Predictive Econometrics/Lecture 5 - Regularised Regression"
#Set the directory you would like to work in
setwd("~/Desktop/HSG/MEcon Herbst Semester/2nd half of the semester/Data Analytics I - Predictive Econometrics/Lecture 5 - Regularised Regression")
#
#Load data
read_csv("R course/Data/browser_2006.csv")
read_csv("R course/Data/browser_new.csv")
#
# ------------------------------------------------------------------------------
#Step 2: Install and load the relevant packages for the problem

#remove # to install the packages
#install.packages("grf")
#install.packages("DiagrammeR")
#install.packages("glmnet")

#load the packages
library("grf")
library("DiagrammeR")
library("glmnet")

#-------------------------------------------------------------------------------

#Exercise 1 - Calculate the average online spending in 2006
mean(browser_2006$spend)

#same code with more text for better comprehension
print(paste("The average online spending of a household in 2006 was", 
            round(mean(browser_2006$spend),digits=2), "USD"))
            
# The average online spending in 2006 is 1959,921 US dollars.

#-------------------------------------------------------------------------------

#Exercise 2 - Find the most popular webpage of household with id 1297

#something is odd with this code, check with Jose
names(browser_2006[browser_2006$id == 1297,-1])[max(browser_2006[browser_2006$id == 1297,-1])]

##### FIRST ALTERNATIVE

id_1297=browser_2006[browser_2006$id == 1297,3:ncol(browser_2006)] #extract row
max(id_1297) # calculate the maximum value in % which represents the
#share of the most visited webpage by the household
sum(id_1297) #should add up to 100

t_id_1297=t(id_1297) #transpose the data frame, so that it can be sorted

id_1297_top_website = head(t_id_1297[order(-t_id_1297),],1) 
# the most visited website

print(paste("The most visited webpage by a household with ID 1297 is",
            names(id_1297_top_website)))

#shortcut function which yields the same result in 1 line

apply(browser_2006[browser_2006$id == 1297,-(1:2)], 1, function(x) max(names(which(x == max(id_1297)))))

##### SECOND ALTERNATIVE

no_id_spend_browser_2006 <- select(browser_2006, -id, -spend)

max <- colnames(no_id_spend_browser_2006)[max.col(no_id_spend_browser_2006,ties.method="first")]

max[1]

# The household with ID=1297 is most of the time on the "weather.com" webpage.

#------------------------------------------------------------------------------

#Exercise 3 - Find two webpages which are the best linear predictors for online
#spending

set.seed(09122022) #for future easier comparision 

lasso_model = cv.glmnet(as.matrix(browser_2006[,-c(1,2)]), browser_2006$spend, 
family = "gaussian", nfolds=5, alpha = 1, dfmax=3) #set the model

vector_coef=c(coef(lasso_model, s = lasso_model$lambda[2])[,1]) 
#lambda such that number of non zero coef is 2, wrap into a vector for simplicity

vector_coef[vector_coef!=0] #all non zero coef plus intercept

#print the names of top 2 websites that are best linear predictors for online spending

top_2=t(vector_coef[vector_coef!=0]) 
names(top_2[,-1])

# The two webpages that are together the best linear predictors for online
# spending in 2006 are "officedepot.com" and "staples.com".

#------------------------------------------------------------------------------

#Exercise 4 - Lasso vs OLS prediction

#Post-lasso model estimated with OLS method
post.lasso_model = lm(spend ~ (officedepot.com + staples.com), data = browser_2006)

#OLS model with all variables
all.variables_ols = lm(spend ~ . , data = browser_2006[,-1]) 

#Predicted Values for new data
prediction.post.lasso_model = predict(post.lasso_model, newdata = browser_new)
prediction.all.varibales_OLS = predict(all.variables_ols, newdata = browser_new)

#Correlation between OLS and Post-Lasso predictions
cor(prediction.all.varibales_OLS, prediction.post.lasso_model) 

#Plot of fitted values 
predicted.values =as.data.frame(cbind(prediction.all.varibales_OLS,prediction.post.lasso_model),
colnames=c("all.variables_ols", "post.lasso_model")) 

ggplot(data = predicted.values, 
aes(x = prediction.all.varibales_OLS, y= prediction.post.lasso_model)) +
geom_point() + 
geom_abline(intercept = 0, slope = 1) +
ggtitle("Scatter Plot with line of best fit with predicted values for test data")+
ylab("Post-Lasso predicted spending") + 
xlab("OLS predicted spending")
      
#Zoomed in plot without some outliers for better visuals
ggplot(data = predicted.values, 
aes(x = prediction.all.varibales_OLS, y= prediction.post.lasso_model)) +
geom_point() + 
geom_abline(intercept = 0, slope = 1) +
xlim(-10000,15000) + 
ylim(1500,10000) +
ggtitle("Zoomed Scatter Plot with line of best fit")+
ylab("Post-Lasso predicted spending") + 
xlab("OLS predicted spending")

#conclusion missing - observing the graph it seems lasso provides more consistent 
# and lower variance predictions - need to justify with/ wether lasso is better

#From the surface, it seems that a post-Lasso model should create more reasonable 
#predictions based on the fact that it generates only positive values. On the other
#hand if OLS based prediction is used then we can see that it can become problematic
#as it would be possible to generate negative predicted values. In essence, it 
#doesn't make sense since spending extra time on a website won't result in customers
#receiving the money for online spending.

#Moreover, judging from the in-sample R-squared values, post-Lasso Model doesn't
#do a great job in explaining the variability in data and therefore it impacts 
#it may impact its ability to predict on out-of-sample data. In our view, 
#it makes sense to include more control variables in order to improve the 
#predictability of post-Lasso model.
